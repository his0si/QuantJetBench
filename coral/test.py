import time
import numpy as np
from PIL import Image # Used for potential resizing
from pycoral.utils.edgetpu import make_interpreter
from pycoral.adapters import classify # Use classify adapter for simplicity
import os
import urllib.request
import tarfile
import pickle

# --- Configuration ---
# List the paths to your Edge TPU compiled TFLite models here
# Make sure these paths are correct on your Jetson device.
# The names should match the files generated by opti.py after edgetpu_compiler.
TFLITE_MODEL_PATHS = [
    #'resnet_hybrid_qat_70percent_qat_edgetpu.tflite',
    'resnet_activation_int8_ptq_edgetpu.tflite',
    'resnet_l2norm_qat_72percent_qat_edgetpu.tflite',
    'resnet_full_qat_100percent_qat_edgetpu.tflite',
    'resnet_middle_qat_56percent_qat_edgetpu.tflite',
    # 필요하다면 다른 모델 경로를 추가하세요.
]

print(f"Will evaluate the following Edge TPU models: {TFLITE_MODEL_PATHS}")

NUM_TEST_IMAGES = 10000 # Evaluate on the full CIFAR-10 test set (10,000 images)
NUM_WARMUP_INFERENCES = 5 # Run a few inferences before timing to warm up the Edge TPU

# --- NEW: Custom CIFAR-10 Data Loader ---
def load_cifar10_test_data(data_dir='cifar10_data'):
    """Loads the CIFAR-10 test dataset manually."""
    url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    tar_filename = os.path.join(data_dir, 'cifar-10-python.tar.gz')
    test_batch_path = os.path.join(data_dir, 'cifar-10-batches-py', 'test_batch')

    # Create data directory if it doesn't exist
    os.makedirs(data_dir, exist_ok=True)

    # Download the dataset if not already downloaded
    if not os.path.exists(tar_filename):
        print(f"Downloading CIFAR-10 dataset from {url} to {tar_filename}...")
        urllib.request.urlretrieve(url, tar_filename)
        print("Download complete.")

    # Extract the dataset if not already extracted (check for a file inside)
    if not os.path.exists(test_batch_path):
        print(f"Extracting {tar_filename}...")
        with tarfile.open(tar_filename, "r:gz") as tar:
            tar.extractall(path=data_dir)
        print("Extraction complete.")

    # Load the test batch
    print(f"Loading test data from {test_batch_path}...")
    with open(test_batch_path, 'rb') as f:
        # CIFAR-10 uses pickle
        batch = pickle.load(f, encoding='bytes')

    # Data is in a 1D numpy array, reshape it to images
    # CIFAR-10 images are 32x32x3, stored row-major (RGBRGB... for 32x32 pixels), then concatenated for all images
    # The original data format is [num_images, 3 * 32 * 32]
    # We need to reshape it to [num_images, 3, 32, 32] and then transpose to [num_images, 32, 32, 3]
    images = batch[b'data'].reshape((10000, 3, 32, 32)).transpose((0, 2, 3, 1))
    labels = np.array(batch[b'labels'])

    print("CIFAR-10 test data loaded.")
    return images, labels

# --- Load Data ---
# Replace tensorflow data loading with our custom function
test_images_raw, test_labels_full = load_cifar10_test_data()
test_images_full = test_images_raw.astype(np.float32) / 255.0 # Normalize to [0, 1]
print(f"Loaded {len(test_images_full)} test images.")

# --- Helper function to preprocess image for the model ---
def preprocess_image(image_float32_0_1, input_details):
    """Preprocesses a float32 [0, 1] image for model input based on input_details."""
    input_shape = input_details['shape']
    input_dtype = input_details['dtype']
    input_scale, input_zero_point = input_details['quantization']

    img_data = image_float32_0_1 # Already normalized to [0, 1]

    # Check if resizing is needed (CIFAR is 32x32, typically matches model input)
    if img_data.shape[:2] != (input_shape[1], input_shape[2]):
        # Resize using PIL (high quality)
        img_pil = Image.fromarray((img_data * 255).astype(np.uint8))
        img_pil = img_pil.resize((input_shape[2], input_shape[1]), Image.Resampling.BICUBIC) # Use BICUBIC for smoother resize
        img_data = np.array(img_pil).astype(np.float32) / 255.0 # Convert back to float [0, 1]
        # print(f"Resized image to {input_shape[1]}x{input_shape[2]}") # Optional: Print resize info

    # Quantize the input data if the model expects integer input (common for Edge TPU)
    if np.issubdtype(input_dtype, np.integer):
        # Apply quantization formula: int = float / scale + zero_point
        # Scale from [0, 1] float to [0, 255] uint8 (or model's specific range)
        # Input scale/zero_point should handle this conversion if model input is UINT8
        # Typical INT8 input expects values scaled like (image_0_255 - zero_point) / scale
        # Since we have [0,1] float, we need to scale it to [0,255] first if input_dtype is uint8
        if input_dtype == np.uint8:
            # Scale float [0,1] to uint8 [0,255]
            img_data_scaled = img_data * 255.0
            # Then apply model's specific quantization if scale/zero_point are non-standard
            # However, for models trained with uint8 input in TFMOT, scale=1/255.0 and zero_point=0 is typical
            # So we can often just scale and cast
            if abs(input_scale - 1.0/255.0) < 1e-6 and input_zero_point == 0:
                # Standard uint8 input
                quantized_input = img_data_scaled.astype(input_dtype)
            else:
                # Apply standard quantization formula (float needs to be in the model's expected float range before this)
                # If model input is uint8, its float range is usually [0, 255]
                quantized_input = (img_data_scaled / input_scale + input_zero_point).astype(input_dtype)
                # Ensure clipping to the dtype range (e.g., 0-255 for uint8)
                quantized_input = np.clip(quantized_input, np.iinfo(input_dtype).min, np.iinfo(input_dtype).max)

        # Handle other integer types if necessary (less common for image input)
        # For INT8, range is [-128, 127], float range might be [-1, 1] or similar
        elif input_dtype == np.int8:
            # Assuming float range is [-1, 1] for example, need proper scaling
            # This would depend heavily on how the model was trained/quantized for INT8 input
            # For now, raise an error as uint8 is most common for images
            raise ValueError(f"Unsupported integer input dtype: {input_dtype} for preprocessing. Expected uint8.")

        else:
            raise ValueError(f"Unsupported integer input dtype: {input_dtype}.")

    else:
        # Input is float, just ensure correct dtype and shape
        quantized_input = img_data.astype(input_dtype)


    # Add batch dimension
    input_tensor = np.expand_dims(quantized_input, axis=0)

    # Verify the final input tensor shape matches the model's expected shape
    if input_tensor.shape != tuple(input_shape):
        raise ValueError(f"Final prepared input tensor shape {input_tensor.shape} does not match model expected input shape {input_shape}")

    return input_tensor


# --- Evaluate models ---
evaluation_results = {} # Store results {model_name: {accuracy: ..., avg_inference_time_ms: ...}}

for model_path in TFLITE_MODEL_PATHS:
    model_name = os.path.basename(model_path)
    print(f"\n--- Evaluating {model_name} on Edge TPU ---")
    time.sleep(0.1)  # Add delay before loading new model

    try:
        # Load the Edge TPU interpreter
        interpreter = make_interpreter(model_path)
        interpreter.allocate_tensors()
        print("Interpreter created and tensors allocated.")
        time.sleep(0.1)  # Add delay after model loading

        input_details = interpreter.get_input_details()[0]
        output_details = interpreter.get_output_details()[0] # Assuming single output

        print(f"Input Tensor Details: {input_details}")
        print(f"Output Tensor Details: {output_details}")
        time.sleep(0.1)  # Add delay after getting tensor details

        # --- Warm-up runs ---
        print(f"Running {NUM_WARMUP_INFERENCES} warm-up inferences...")
        if NUM_TEST_IMAGES > 0:
            dummy_image = test_images_full[0] # Use the first image for warm-up
            try:
                for _ in range(NUM_WARMUP_INFERENCES):
                    input_tensor = preprocess_image(dummy_image, input_details)
                    interpreter.set_tensor(input_details['index'], input_tensor)
                    interpreter.invoke()
                    # Optionally read output to ensure full pipeline runs
                    # Use get_tensor instead of classify.get_classes for warm-up to be general
                    _ = interpreter.get_tensor(output_details['index'])
            except Exception as e:
                print(f"Warning: Warm-up inference failed: {e}")
                import traceback
                traceback.print_exc()
                # Continue with evaluation, but be aware first few timed inferences might be slower
        else:
            print("Skipping warm-up as no test images are available.")


        # --- Timed Inference and Accuracy Evaluation ---
        print(f"Starting timed inference and accuracy evaluation on {NUM_TEST_IMAGES} images...")
        correct_predictions = 0
        inference_times_ms = []
        processed_images_count = 0 # Track successfully processed images

        start_time = time.perf_counter()

        # Ensure we only iterate up to NUM_TEST_IMAGES, even if test_images_full is smaller
        for i in range(min(NUM_TEST_IMAGES, len(test_images_full))):
            if i > 0 and i % 100 == 0:  # Add delay every 100 images
                time.sleep(0.1)
                
            image = test_images_full[i]
            true_label = test_labels_full[i]

            try:
                # Preprocess image
                input_tensor = preprocess_image(image, input_details)

                # Perform inference
                inference_start_time = time.perf_counter()
                interpreter.set_tensor(input_details['index'], input_tensor)
                interpreter.invoke()
                inference_end_time = time.perf_counter()
                time.sleep(0.1)  # Add delay after each inference

                # Record inference time for this image
                inference_times_ms.append((inference_end_time - inference_start_time) * 1000)

                # Get prediction using pycoral classify adapter (handles dequantization)
                # Ensure top_k is at least 1 to get the top prediction
                # classify.get_classes expects output tensor shape (1, num_classes)
                # and INT8/UINT8 output dtype.
                if output_details['dtype'] in [np.int8, np.uint8] and len(output_details['shape']) == 2 and output_details['shape'][0] == 1:
                    results = classify.get_classes(interpreter, top_k=1)
                    if results:
                        predicted_class_id = results[0][0] # get_classes returns list of (class_id, score) tuples
                        if predicted_class_id == true_label:
                            correct_predictions += 1
                    else:
                        # Handle case where classify.get_classes returns empty (e.g., if output is weird)
                        print(f"Warning: No classification results from adapter for image {i}")
                else:
                    # Manual output processing if adapter is not suitable
                    output_data = interpreter.get_tensor(output_details['index'])[0] # Get output, remove batch dim
                    # Manually dequantize if output is integer
                    if np.issubdtype(output_details['dtype'], np.integer):
                        output_scale, output_zero_point = output_details['quantization']
                        output_scale = output_scale if output_scale != 0 else 1e-8
                        dequantized_output = (output_data.astype(np.float32) - output_zero_point) * output_scale
                        pred = np.argmax(dequantized_output)
                    else:
                        # Float output (unlikely for Edge TPU but handle)
                        pred = np.argmax(output_data.astype(np.float32))

                    predicted_class_id = pred
                    if predicted_class_id == true_label:
                        correct_predictions += 1

                processed_images_count += 1 # Increment count for successfully processed images

            except Exception as e:
                print(f"\nError during inference or evaluation for image {i}: {e}")
                import traceback
                traceback.print_exc()
                # Decide how to handle errors during evaluation - skip the image or stop?
                # Let's skip the image and continue for now
                continue # Skip to the next image

            # Optional: Print progress
            if (i + 1) % 100 == 0:
                print(f"Processed {processed_images_count}/{min(NUM_TEST_IMAGES, len(test_images_full))} images...", end='\r')


        end_time = time.perf_counter()
        total_inference_time = end_time - start_time
        # Use processed_images_count for average time and accuracy calculation
        avg_inference_time_ms = (sum(inference_times_ms) / processed_images_count) if processed_images_count > 0 else 0
        accuracy = correct_predictions / processed_images_count if processed_images_count > 0 else 0

        # Clear the progress line
        print(" " * 50, end='\r')

        print(f"Evaluation complete for {model_name}.")
        print(f"  Images Processed: {processed_images_count}/{min(NUM_TEST_IMAGES, len(test_images_full))}")
        print(f"  Accuracy: {accuracy*100:.2f}% ({correct_predictions}/{processed_images_count})")
        # Only print timing if at least one image was successfully processed
        if processed_images_count > 0:
            print(f"  Total Inference Time: {total_inference_time:.4f} seconds")
            print(f"  Average Inference Time: {avg_inference_time_ms:.4f} ms/image")
        else:
            print("  Timing data not available (no images processed).")


        evaluation_results[model_name] = {
            'accuracy': accuracy,
            'avg_inference_time_ms': avg_inference_time_ms,
            'total_inference_time_sec': total_inference_time,
            'processed_images': processed_images_count
        }

    except Exception as e:
        print(f"\nError loading or evaluating model {model_name}: {e}")
        import traceback
        traceback.print_exc()
        evaluation_results[model_name] = {'accuracy': None, 'avg_inference_time_ms': None, 'total_inference_time_sec': None, 'processed_images': 0}
        print(f"Skipping {model_name} due to error.")


# --- Summary of Results ---
print("\n--- Edge TPU Evaluation Summary ---")
if evaluation_results:
    # Sort results for cleaner output (e.g., by accuracy descending)
    # Handle cases where evaluation failed (accuracy is None)
    sorted_results = sorted(evaluation_results.items(), key=lambda item: item[1].get('accuracy', -1) if item[1] and item[1].get('accuracy') is not None else -2, reverse=True)

    for model_name, results in sorted_results:
        print(f"\nModel: {model_name}")
        if results and results['accuracy'] is not None:
            print(f"  Accuracy: {results['accuracy']*100:.2f}%")
            print(f"  Processed Images: {results['processed_images']}/{min(NUM_TEST_IMAGES, len(test_images_full))}")
            if results['processed_images'] > 0:
                 print(f"  Average Inference Time: {results['avg_inference_time_ms']:.4f} ms/image")
                 print(f"  Total Inference Time ({results['processed_images']} images): {results['total_inference_time_sec']:.4f} seconds")
            else:
                 print("  Timing data not available (no images processed).")
        else:
            print("  Evaluation Failed")
else:
    print("No models were successfully evaluated.")

print("\nEdge TPU evaluation script finished.")